[2022-03-04 11:52:34,866] [WARNING] [runner.py:148:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2022-03-04 11:52:34,924] [INFO] [runner.py:420:main] cmd = /data0/zxh/anaconda3/envs/XDAI/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=60531 finetune_glm.py --deepspeed --deepspeed_config config_tasks/config_blocklm_10B_cnndm.json --finetune --experiment-name blocklm-10B-cnndm_org_03-04-11-52 --task cnn_dm_original --data-dir /data0/xyf/tunedata --save /data0/xyf --checkpoint-activations --num-workers 1 --no-load-lr-scheduler --block-lm --cloze-eval --task-mask --num-layers 48 --hidden-size 4096 --num-attention-heads 64 --max-position-embeddings 1024 --tokenizer-type ChineseSPTokenizer --load-pretrained /data0/xyf/blocklm_10B_chinese --epochs 10 --batch-size 8 --lr 1e-5 --lr-decay-style linear --warmup 0.06 --weight-decay 1.0e-1 --label-smoothing 0.1 --save-interval 10000 --log-interval 50 --eval-interval 1000 --eval-iters 100 --eval-epoch 2 --src-seq-length 608 --tgt-seq-length 160 --min-tgt-length 55 --length-penalty 0.7 --no-repeat-ngram-size 3 --num-beams 5 --select-topk --eval-batch-size 1 --fp16 --model-parallel-size 1 --overwrite
[2022-03-04 11:52:35,703] [INFO] [launch.py:89:main] 0 NCCL_DEBUG=info
[2022-03-04 11:52:35,704] [INFO] [launch.py:89:main] 0 NCCL_NET_GDR_LEVEL=2
[2022-03-04 11:52:35,704] [INFO] [launch.py:89:main] 0 NCCL_IB_DISABLE=0
[2022-03-04 11:52:35,704] [INFO] [launch.py:96:main] WORLD INFO DICT: {'localhost': [0]}
[2022-03-04 11:52:35,704] [INFO] [launch.py:102:main] nnodes=1, num_local_procs=1, node_rank=0
[2022-03-04 11:52:35,704] [INFO] [launch.py:115:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2022-03-04 11:52:35,704] [INFO] [launch.py:116:main] dist_world_size=1
[2022-03-04 11:52:35,704] [INFO] [launch.py:118:main] Setting CUDA_VISIBLE_DEVICES=0
using world size: 1 and model-parallel size: 1 
 > using dynamic loss scaling
172-31-255-12:16511:16511 [0] NCCL INFO Bootstrap : Using [0]bond0:10.0.0.234<0> [1]vethe78397e:fe80::98ec:45ff:feb1:c82e%vethe78397e<0> [2]veth0f29499:fe80::b0fd:e1ff:feda:638e%veth0f29499<0>
172-31-255-12:16511:16511 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
172-31-255-12:16511:16511 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
172-31-255-12:16511:16511 [0] NCCL INFO NET/IB : Using [0]mlx5_bond_0:1/RoCE ; OOB bond0:10.0.0.234<0>
172-31-255-12:16511:16511 [0] NCCL INFO Using network IB
NCCL version 2.7.8+cuda10.2
172-31-255-12:16511:16671 [0] NCCL INFO Channel 00/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 01/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 02/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 03/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 04/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 05/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 06/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 07/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 08/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 09/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 10/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 11/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 12/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 13/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 14/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 15/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 16/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 17/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 18/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 19/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 20/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 21/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 22/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 23/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 24/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 25/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 26/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 27/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 28/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 29/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 30/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Channel 31/32 :    0
172-31-255-12:16511:16671 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->
172-31-255-12:16511:16671 [0] NCCL INFO Setting affinity for GPU 2 to 3ffff0,0003ffff
172-31-255-12:16511:16671 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
172-31-255-12:16511:16671 [0] NCCL INFO comm 0x7ff594000e00 rank 0 nranks 1 cudaDev 0 busId 1d000 - Init COMPLETE
> initializing model parallel with size 1
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
> padded vocab (size: 50009) with 39 dummy tokens (new size: 50048)
> found end-of-document token: 50000
172-31-255-12:16511:16676 [0] NCCL INFO Channel 00/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 01/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 02/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 03/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 04/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 05/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 06/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 07/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 08/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 09/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 10/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 11/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 12/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 13/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 14/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 15/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 16/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 17/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 18/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 19/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 20/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 21/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 22/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 23/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 24/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 25/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 26/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 27/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 28/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 29/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 30/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Channel 31/32 :    0
172-31-255-12:16511:16676 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->
172-31-255-12:16511:16676 [0] NCCL INFO Setting affinity for GPU 2 to 3ffff0,0003ffff
172-31-255-12:16511:16676 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
172-31-255-12:16511:16676 [0] NCCL INFO comm 0x7ff59c000e50 rank 0 nranks 1 cudaDev 0 busId 1d000 - Init COMPLETE
Creating cnn_dm_original-train dataset from /data0/xyf/tunedata
Traceback (most recent call last):
  File "finetune_glm.py", line 470, in <module>
    main(args)
  File "/home/XuYifan/XDAI/tools/PLM/GLM/tasks/seq2seq/finetune.py", line 122, in main
    finetune(args, train_valid_datasets_provider, {}, end_of_epoch_callback_provider=metrics_func_provider,
  File "/home/XuYifan/XDAI/tools/PLM/GLM/finetune_glm.py", line 298, in finetune
    train_dataset, valid_dataset = train_valid_datasets_provider(args, tokenizer)
  File "/home/XuYifan/XDAI/tools/PLM/GLM/tasks/seq2seq/finetune.py", line 63, in train_valid_datasets_provider
    train_dataset = Seq2SeqDataset(args, split='train', tokenizer=tokenizer)
  File "/home/XuYifan/XDAI/tools/PLM/GLM/tasks/seq2seq/dataset.py", line 224, in __init__
    example_list = self.processor.create_examples(split)
  File "/home/XuYifan/XDAI/tools/PLM/GLM/tasks/seq2seq/dataset.py", line 81, in create_examples
    with open(os.path.join(self.data_dir, f"{filename}.source"), encoding='utf-8') as file:
FileNotFoundError: [Errno 2] No such file or directory: '/data0/xyf/tunedata/train.source'
[2022-03-04 11:52:41,716] [INFO] [launch.py:160:sigkill_handler] Killing subprocess 16511
[2022-03-04 11:52:41,717] [ERROR] [launch.py:166:sigkill_handler] ['/data0/zxh/anaconda3/envs/XDAI/bin/python', '-u', 'finetune_glm.py', '--local_rank=0', '--deepspeed', '--deepspeed_config', 'config_tasks/config_blocklm_10B_cnndm.json', '--finetune', '--experiment-name', 'blocklm-10B-cnndm_org_03-04-11-52', '--task', 'cnn_dm_original', '--data-dir', '/data0/xyf/tunedata', '--save', '/data0/xyf', '--checkpoint-activations', '--num-workers', '1', '--no-load-lr-scheduler', '--block-lm', '--cloze-eval', '--task-mask', '--num-layers', '48', '--hidden-size', '4096', '--num-attention-heads', '64', '--max-position-embeddings', '1024', '--tokenizer-type', 'ChineseSPTokenizer', '--load-pretrained', '/data0/xyf/blocklm_10B_chinese', '--epochs', '10', '--batch-size', '8', '--lr', '1e-5', '--lr-decay-style', 'linear', '--warmup', '0.06', '--weight-decay', '1.0e-1', '--label-smoothing', '0.1', '--save-interval', '10000', '--log-interval', '50', '--eval-interval', '1000', '--eval-iters', '100', '--eval-epoch', '2', '--src-seq-length', '608', '--tgt-seq-length', '160', '--min-tgt-length', '55', '--length-penalty', '0.7', '--no-repeat-ngram-size', '3', '--num-beams', '5', '--select-topk', '--eval-batch-size', '1', '--fp16', '--model-parallel-size', '1', '--overwrite'] exits with return code = 1
